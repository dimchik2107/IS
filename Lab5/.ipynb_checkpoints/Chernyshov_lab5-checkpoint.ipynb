{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fc3fb7",
   "metadata": {},
   "source": [
    "# Лабораторная работа № 5\n",
    "\n",
    "Распознавание объектов на фотографиях\n",
    "Выполнил:\n",
    "    Студент группы БФИ1901\n",
    "    Чернышов Дмитрий\n",
    "    \n",
    "Задачи:\n",
    "\n",
    "   1. Ознакомиться со сверточными нейронными сетями\n",
    "   2. Изучить построение модели в Keras в функциональном виде\n",
    "   3. Изучить работу слоя разреживания (Dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e581aba",
   "metadata": {},
   "source": [
    "# Цель работы:\n",
    "Распознавание объектов на фотографиях (Object Recognition in Photographs)\n",
    "CIFAR-10 (классификация небольших изображений по десяти классам: самолет,\n",
    "автомобиль, птица, кошка, олень, собака, лягушка, лошадь, корабль и грузовик)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad17c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.utils import np_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acfd8f4-5f98-4f85-9862-41b4dddcfd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 427s 3us/step\n",
      "170508288/170498071 [==============================] - 427s 3us/step\n",
      "Epoch 1/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.8910 - accuracy: 0.2856 - val_loss: 1.6373 - val_accuracy: 0.3800\n",
      "Epoch 2/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.6760 - accuracy: 0.3790 - val_loss: 1.5209 - val_accuracy: 0.4398\n",
      "Epoch 3/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.5966 - accuracy: 0.4080 - val_loss: 1.4535 - val_accuracy: 0.4744\n",
      "Epoch 4/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.5476 - accuracy: 0.4299 - val_loss: 1.4115 - val_accuracy: 0.4866\n",
      "Epoch 5/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.5131 - accuracy: 0.4433 - val_loss: 1.4308 - val_accuracy: 0.4778\n",
      "Epoch 6/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.4775 - accuracy: 0.4600 - val_loss: 1.3546 - val_accuracy: 0.5020\n",
      "Epoch 7/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.4532 - accuracy: 0.4722 - val_loss: 1.3415 - val_accuracy: 0.5188\n",
      "Epoch 8/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4360 - accuracy: 0.4766 - val_loss: 1.3253 - val_accuracy: 0.5228\n",
      "Epoch 9/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.4203 - accuracy: 0.4839 - val_loss: 1.3195 - val_accuracy: 0.5286\n",
      "Epoch 10/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.4057 - accuracy: 0.4937 - val_loss: 1.3501 - val_accuracy: 0.5118\n",
      "Epoch 11/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3912 - accuracy: 0.4950 - val_loss: 1.3050 - val_accuracy: 0.5328\n",
      "Epoch 12/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.3776 - accuracy: 0.5036 - val_loss: 1.2887 - val_accuracy: 0.5412\n",
      "Epoch 13/200\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 1.3652 - accuracy: 0.5094 - val_loss: 1.2687 - val_accuracy: 0.5450\n",
      "Epoch 14/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3576 - accuracy: 0.5118 - val_loss: 1.2715 - val_accuracy: 0.5478\n",
      "Epoch 15/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3553 - accuracy: 0.5134 - val_loss: 1.2701 - val_accuracy: 0.5464\n",
      "Epoch 16/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3388 - accuracy: 0.5211 - val_loss: 1.2484 - val_accuracy: 0.5568\n",
      "Epoch 17/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3367 - accuracy: 0.5186 - val_loss: 1.2445 - val_accuracy: 0.5530\n",
      "Epoch 18/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3264 - accuracy: 0.5241 - val_loss: 1.2704 - val_accuracy: 0.5464\n",
      "Epoch 19/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3173 - accuracy: 0.5283 - val_loss: 1.2586 - val_accuracy: 0.5502\n",
      "Epoch 20/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3128 - accuracy: 0.5302 - val_loss: 1.2597 - val_accuracy: 0.5502\n",
      "Epoch 21/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3068 - accuracy: 0.5335 - val_loss: 1.2496 - val_accuracy: 0.5502\n",
      "Epoch 22/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2994 - accuracy: 0.5334 - val_loss: 1.2191 - val_accuracy: 0.5704\n",
      "Epoch 23/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2986 - accuracy: 0.5338 - val_loss: 1.2387 - val_accuracy: 0.5520\n",
      "Epoch 24/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.2964 - accuracy: 0.5366 - val_loss: 1.2405 - val_accuracy: 0.5570\n",
      "Epoch 25/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.2856 - accuracy: 0.5377 - val_loss: 1.2514 - val_accuracy: 0.5650\n",
      "Epoch 26/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2845 - accuracy: 0.5403 - val_loss: 1.2192 - val_accuracy: 0.5712\n",
      "Epoch 27/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2798 - accuracy: 0.5440 - val_loss: 1.2118 - val_accuracy: 0.5682\n",
      "Epoch 28/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2722 - accuracy: 0.5449 - val_loss: 1.2049 - val_accuracy: 0.5720\n",
      "Epoch 29/200\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 1.2744 - accuracy: 0.5439 - val_loss: 1.2750 - val_accuracy: 0.5568\n",
      "Epoch 30/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2698 - accuracy: 0.5451 - val_loss: 1.2231 - val_accuracy: 0.5546\n",
      "Epoch 31/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2656 - accuracy: 0.5460 - val_loss: 1.2080 - val_accuracy: 0.5738\n",
      "Epoch 32/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2621 - accuracy: 0.5510 - val_loss: 1.2074 - val_accuracy: 0.5710\n",
      "Epoch 33/200\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.2568 - accuracy: 0.5497 - val_loss: 1.1894 - val_accuracy: 0.5774\n",
      "Epoch 34/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2559 - accuracy: 0.5509 - val_loss: 1.2166 - val_accuracy: 0.5726\n",
      "Epoch 35/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2554 - accuracy: 0.5537 - val_loss: 1.2258 - val_accuracy: 0.5604\n",
      "Epoch 36/200\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2596 - accuracy: 0.5505 - val_loss: 1.2041 - val_accuracy: 0.5662\n",
      "Epoch 37/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2536 - accuracy: 0.5498 - val_loss: 1.2035 - val_accuracy: 0.5702\n",
      "Epoch 38/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2523 - accuracy: 0.5539 - val_loss: 1.2160 - val_accuracy: 0.5720\n",
      "Epoch 39/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2452 - accuracy: 0.5542 - val_loss: 1.2058 - val_accuracy: 0.5656\n",
      "Epoch 40/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2452 - accuracy: 0.5547 - val_loss: 1.1942 - val_accuracy: 0.5780\n",
      "Epoch 41/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2394 - accuracy: 0.5580 - val_loss: 1.2020 - val_accuracy: 0.5764\n",
      "Epoch 42/200\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 1.2383 - accuracy: 0.5562 - val_loss: 1.1938 - val_accuracy: 0.5768\n",
      "Epoch 43/200\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 1.2362 - accuracy: 0.5592 - val_loss: 1.2255 - val_accuracy: 0.5568\n",
      "Epoch 44/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2313 - accuracy: 0.5600 - val_loss: 1.1833 - val_accuracy: 0.5802\n",
      "Epoch 45/200\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2345 - accuracy: 0.5587 - val_loss: 1.2488 - val_accuracy: 0.5590\n",
      "Epoch 46/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2396 - accuracy: 0.5577 - val_loss: 1.1989 - val_accuracy: 0.5750\n",
      "Epoch 47/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2316 - accuracy: 0.5607 - val_loss: 1.2015 - val_accuracy: 0.5744\n",
      "Epoch 48/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2348 - accuracy: 0.5608 - val_loss: 1.1924 - val_accuracy: 0.5770\n",
      "Epoch 49/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.2256 - accuracy: 0.5630 - val_loss: 1.1973 - val_accuracy: 0.5740\n",
      "Epoch 50/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.2279 - accuracy: 0.5581 - val_loss: 1.2040 - val_accuracy: 0.5730\n",
      "Epoch 51/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2220 - accuracy: 0.5652 - val_loss: 1.2097 - val_accuracy: 0.5710\n",
      "Epoch 52/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2240 - accuracy: 0.5631 - val_loss: 1.2071 - val_accuracy: 0.5730\n",
      "Epoch 53/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2294 - accuracy: 0.5628 - val_loss: 1.2075 - val_accuracy: 0.5654\n",
      "Epoch 54/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2262 - accuracy: 0.5621 - val_loss: 1.2271 - val_accuracy: 0.5632\n",
      "Epoch 55/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2212 - accuracy: 0.5629 - val_loss: 1.1928 - val_accuracy: 0.5726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2244 - accuracy: 0.5632 - val_loss: 1.2236 - val_accuracy: 0.5688\n",
      "Epoch 57/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2155 - accuracy: 0.5677 - val_loss: 1.2175 - val_accuracy: 0.5598\n",
      "Epoch 58/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2104 - accuracy: 0.5688 - val_loss: 1.1940 - val_accuracy: 0.5830\n",
      "Epoch 59/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2203 - accuracy: 0.5650 - val_loss: 1.2054 - val_accuracy: 0.5804\n",
      "Epoch 60/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2112 - accuracy: 0.5701 - val_loss: 1.1907 - val_accuracy: 0.5786\n",
      "Epoch 61/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2113 - accuracy: 0.5688 - val_loss: 1.2002 - val_accuracy: 0.5776\n",
      "Epoch 62/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2173 - accuracy: 0.5654 - val_loss: 1.2116 - val_accuracy: 0.5766\n",
      "Epoch 63/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2118 - accuracy: 0.5699 - val_loss: 1.1967 - val_accuracy: 0.5792\n",
      "Epoch 64/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.2062 - accuracy: 0.5704 - val_loss: 1.2043 - val_accuracy: 0.5734\n",
      "Epoch 65/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.2084 - accuracy: 0.5687 - val_loss: 1.2179 - val_accuracy: 0.5672\n",
      "Epoch 66/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.2101 - accuracy: 0.5667 - val_loss: 1.1914 - val_accuracy: 0.5812\n",
      "Epoch 67/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.2093 - accuracy: 0.5710 - val_loss: 1.2058 - val_accuracy: 0.5834\n",
      "Epoch 68/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.2063 - accuracy: 0.5687 - val_loss: 1.2112 - val_accuracy: 0.5756\n",
      "Epoch 69/200\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 1.2075 - accuracy: 0.5711 - val_loss: 1.2101 - val_accuracy: 0.5712\n",
      "Epoch 70/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2127 - accuracy: 0.5677 - val_loss: 1.1970 - val_accuracy: 0.5754\n",
      "Epoch 71/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1994 - accuracy: 0.5751 - val_loss: 1.2181 - val_accuracy: 0.5740\n",
      "Epoch 72/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2050 - accuracy: 0.5721 - val_loss: 1.1749 - val_accuracy: 0.5828\n",
      "Epoch 73/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2056 - accuracy: 0.5718 - val_loss: 1.2010 - val_accuracy: 0.5720\n",
      "Epoch 74/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2019 - accuracy: 0.5744 - val_loss: 1.2244 - val_accuracy: 0.5730\n",
      "Epoch 75/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1995 - accuracy: 0.5717 - val_loss: 1.2067 - val_accuracy: 0.5776\n",
      "Epoch 76/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1960 - accuracy: 0.5746 - val_loss: 1.1965 - val_accuracy: 0.5798\n",
      "Epoch 77/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1987 - accuracy: 0.5733 - val_loss: 1.2027 - val_accuracy: 0.5658\n",
      "Epoch 78/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2005 - accuracy: 0.5718 - val_loss: 1.1868 - val_accuracy: 0.5778\n",
      "Epoch 79/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2012 - accuracy: 0.5766 - val_loss: 1.1836 - val_accuracy: 0.5786\n",
      "Epoch 80/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1957 - accuracy: 0.5799 - val_loss: 1.2053 - val_accuracy: 0.5742\n",
      "Epoch 81/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1951 - accuracy: 0.5779 - val_loss: 1.1845 - val_accuracy: 0.5810\n",
      "Epoch 82/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1975 - accuracy: 0.5764 - val_loss: 1.1833 - val_accuracy: 0.5810\n",
      "Epoch 83/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1948 - accuracy: 0.5759 - val_loss: 1.2142 - val_accuracy: 0.5786\n",
      "Epoch 84/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1923 - accuracy: 0.5764 - val_loss: 1.2009 - val_accuracy: 0.5808\n",
      "Epoch 85/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1906 - accuracy: 0.5762 - val_loss: 1.2024 - val_accuracy: 0.5822\n",
      "Epoch 86/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1910 - accuracy: 0.5774 - val_loss: 1.1924 - val_accuracy: 0.5814\n",
      "Epoch 87/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1879 - accuracy: 0.5808 - val_loss: 1.1918 - val_accuracy: 0.5828\n",
      "Epoch 88/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1922 - accuracy: 0.5792 - val_loss: 1.1848 - val_accuracy: 0.5780\n",
      "Epoch 89/200\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1872 - accuracy: 0.5795 - val_loss: 1.1907 - val_accuracy: 0.5850\n",
      "Epoch 90/200\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1859 - accuracy: 0.5807 - val_loss: 1.1937 - val_accuracy: 0.5818\n",
      "Epoch 91/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1928 - accuracy: 0.5788 - val_loss: 1.1977 - val_accuracy: 0.5766\n",
      "Epoch 92/200\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 1.1898 - accuracy: 0.5786 - val_loss: 1.1795 - val_accuracy: 0.5884\n",
      "Epoch 93/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1913 - accuracy: 0.5805 - val_loss: 1.1979 - val_accuracy: 0.5740\n",
      "Epoch 94/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1815 - accuracy: 0.5780 - val_loss: 1.1844 - val_accuracy: 0.5756\n",
      "Epoch 95/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1890 - accuracy: 0.5792 - val_loss: 1.1817 - val_accuracy: 0.5800\n",
      "Epoch 96/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1881 - accuracy: 0.5785 - val_loss: 1.1998 - val_accuracy: 0.5856\n",
      "Epoch 97/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1874 - accuracy: 0.5771 - val_loss: 1.1943 - val_accuracy: 0.5840\n",
      "Epoch 98/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1900 - accuracy: 0.5781 - val_loss: 1.1875 - val_accuracy: 0.5810\n",
      "Epoch 99/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1889 - accuracy: 0.5783 - val_loss: 1.1936 - val_accuracy: 0.5794\n",
      "Epoch 100/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1893 - accuracy: 0.5787 - val_loss: 1.1989 - val_accuracy: 0.5734\n",
      "Epoch 101/200\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 1.1908 - accuracy: 0.5776 - val_loss: 1.1856 - val_accuracy: 0.5774\n",
      "Epoch 102/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.1803 - accuracy: 0.5804 - val_loss: 1.1963 - val_accuracy: 0.5740\n",
      "Epoch 103/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.1895 - accuracy: 0.5761 - val_loss: 1.1860 - val_accuracy: 0.5762\n",
      "Epoch 104/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1825 - accuracy: 0.5802 - val_loss: 1.2064 - val_accuracy: 0.5756\n",
      "Epoch 105/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.1823 - accuracy: 0.5821 - val_loss: 1.1982 - val_accuracy: 0.5830\n",
      "Epoch 106/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1831 - accuracy: 0.5771 - val_loss: 1.1856 - val_accuracy: 0.5910\n",
      "Epoch 107/200\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 1.1791 - accuracy: 0.5816 - val_loss: 1.2043 - val_accuracy: 0.5780\n",
      "Epoch 108/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1743 - accuracy: 0.5849 - val_loss: 1.1837 - val_accuracy: 0.5814\n",
      "Epoch 109/200\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1798 - accuracy: 0.5795 - val_loss: 1.1786 - val_accuracy: 0.5820\n",
      "Epoch 110/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1755 - accuracy: 0.5812 - val_loss: 1.1996 - val_accuracy: 0.5772\n",
      "Epoch 111/200\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1823 - accuracy: 0.5827 - val_loss: 1.1966 - val_accuracy: 0.5882\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1821 - accuracy: 0.5815 - val_loss: 1.1972 - val_accuracy: 0.5764\n",
      "Epoch 113/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1850 - accuracy: 0.5809 - val_loss: 1.1983 - val_accuracy: 0.5710\n",
      "Epoch 114/200\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.1747 - accuracy: 0.5818 - val_loss: 1.2191 - val_accuracy: 0.5688\n",
      "Epoch 115/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1779 - accuracy: 0.5817 - val_loss: 1.1810 - val_accuracy: 0.5892\n",
      "Epoch 116/200\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.1804 - accuracy: 0.5797 - val_loss: 1.2273 - val_accuracy: 0.5644\n",
      "Epoch 117/200\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1739 - accuracy: 0.5842 - val_loss: 1.1978 - val_accuracy: 0.5798\n",
      "Epoch 118/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1738 - accuracy: 0.5810 - val_loss: 1.1893 - val_accuracy: 0.5804\n",
      "Epoch 119/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.1745 - accuracy: 0.5796 - val_loss: 1.1922 - val_accuracy: 0.5888\n",
      "Epoch 120/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1734 - accuracy: 0.5867 - val_loss: 1.1869 - val_accuracy: 0.5754\n",
      "Epoch 121/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1812 - accuracy: 0.5811 - val_loss: 1.2051 - val_accuracy: 0.5820\n",
      "Epoch 122/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1774 - accuracy: 0.5818 - val_loss: 1.2080 - val_accuracy: 0.5784\n",
      "Epoch 123/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1746 - accuracy: 0.5853 - val_loss: 1.1831 - val_accuracy: 0.5802\n",
      "Epoch 124/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1766 - accuracy: 0.5812 - val_loss: 1.1773 - val_accuracy: 0.5894\n",
      "Epoch 125/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1752 - accuracy: 0.5843 - val_loss: 1.1815 - val_accuracy: 0.5872\n",
      "Epoch 126/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1685 - accuracy: 0.5860 - val_loss: 1.1809 - val_accuracy: 0.5854\n",
      "Epoch 127/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1717 - accuracy: 0.5866 - val_loss: 1.1974 - val_accuracy: 0.5746\n",
      "Epoch 128/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1694 - accuracy: 0.5859 - val_loss: 1.2089 - val_accuracy: 0.5766\n",
      "Epoch 129/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1741 - accuracy: 0.5849 - val_loss: 1.1806 - val_accuracy: 0.5820\n",
      "Epoch 130/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1700 - accuracy: 0.5836 - val_loss: 1.2036 - val_accuracy: 0.5764\n",
      "Epoch 131/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1739 - accuracy: 0.5840 - val_loss: 1.1816 - val_accuracy: 0.5876\n",
      "Epoch 132/200\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1712 - accuracy: 0.5852 - val_loss: 1.1951 - val_accuracy: 0.5820\n",
      "Epoch 133/200\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1665 - accuracy: 0.5856 - val_loss: 1.1925 - val_accuracy: 0.5808\n",
      "Epoch 134/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1742 - accuracy: 0.5833 - val_loss: 1.1915 - val_accuracy: 0.5828\n",
      "Epoch 135/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1669 - accuracy: 0.5864 - val_loss: 1.1661 - val_accuracy: 0.5892\n",
      "Epoch 136/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1701 - accuracy: 0.5868 - val_loss: 1.1849 - val_accuracy: 0.5910\n",
      "Epoch 137/200\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 1.1626 - accuracy: 0.5887 - val_loss: 1.1905 - val_accuracy: 0.5830\n",
      "Epoch 138/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1657 - accuracy: 0.5868 - val_loss: 1.2227 - val_accuracy: 0.5718\n",
      "Epoch 139/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1658 - accuracy: 0.5868 - val_loss: 1.1975 - val_accuracy: 0.5798\n",
      "Epoch 140/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1638 - accuracy: 0.5879 - val_loss: 1.1874 - val_accuracy: 0.5820\n",
      "Epoch 141/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1648 - accuracy: 0.5868 - val_loss: 1.1861 - val_accuracy: 0.5882\n",
      "Epoch 142/200\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1697 - accuracy: 0.5859 - val_loss: 1.1812 - val_accuracy: 0.5850\n",
      "Epoch 143/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1651 - accuracy: 0.5889 - val_loss: 1.1819 - val_accuracy: 0.5786\n",
      "Epoch 144/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1666 - accuracy: 0.5858 - val_loss: 1.1898 - val_accuracy: 0.5804\n",
      "Epoch 145/200\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1613 - accuracy: 0.5895 - val_loss: 1.1895 - val_accuracy: 0.5868\n",
      "Epoch 146/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.1670 - accuracy: 0.5869 - val_loss: 1.1908 - val_accuracy: 0.5788\n",
      "Epoch 147/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1631 - accuracy: 0.5880 - val_loss: 1.1877 - val_accuracy: 0.5852\n",
      "Epoch 148/200\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1611 - accuracy: 0.5892 - val_loss: 1.1828 - val_accuracy: 0.5828\n",
      "Epoch 149/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1653 - accuracy: 0.5880 - val_loss: 1.1747 - val_accuracy: 0.5858\n",
      "Epoch 150/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1664 - accuracy: 0.5874 - val_loss: 1.1968 - val_accuracy: 0.5862\n",
      "Epoch 151/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1593 - accuracy: 0.5905 - val_loss: 1.1948 - val_accuracy: 0.5810\n",
      "Epoch 152/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1599 - accuracy: 0.5912 - val_loss: 1.2100 - val_accuracy: 0.5716\n",
      "Epoch 153/200\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 1.1615 - accuracy: 0.5878 - val_loss: 1.2024 - val_accuracy: 0.5740\n",
      "Epoch 154/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1657 - accuracy: 0.5894 - val_loss: 1.1858 - val_accuracy: 0.5842\n",
      "Epoch 155/200\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1639 - accuracy: 0.5888 - val_loss: 1.1791 - val_accuracy: 0.5878\n",
      "Epoch 156/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.1602 - accuracy: 0.5900 - val_loss: 1.1934 - val_accuracy: 0.5790\n",
      "Epoch 157/200\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.1615 - accuracy: 0.5887 - val_loss: 1.1833 - val_accuracy: 0.5850\n",
      "Epoch 158/200\n",
      " 252/1407 [====>.........................] - ETA: 4s - loss: 1.1543 - accuracy: 0.5898"
     ]
    }
   ],
   "source": [
    "batch_size = 32 \n",
    "num_epochs = 200 \n",
    "kernel_size = 3 # использование ядра 3x3 \n",
    "pool_size = 2 #   использование объединения 2х2 \n",
    "conv_depth_1 = 32 #  32 ядра на слой преобразования\n",
    "conv_depth_2 = 64 # переход на 64 после первого уровня объединения\n",
    "drop_prob_1 = 0.25 #  вероятность 0,25\n",
    "drop_prob_2 = 0.5 #  в плотном слое вероятность 0,5\n",
    "hidden_size = 512 # в плотном слое 512 нейронов\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() \n",
    "num_train, depth, height, width = X_train.shape \n",
    "num_test = X_test.shape[0] \n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= np.max(X_train)\n",
    "X_test /= np.max(X_train) \n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) \n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) \n",
    "\n",
    "\n",
    "inp = Input(shape=(depth, height, width)) # N.B. глубина занимает первое место в Керасе\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(inp) #устарело\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)  #Для предотвращения переобучения\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size),padding='same')(conv_4) # padding - входное изображение должно \n",
    "#иметь нулевое заполнение, чтобы вывод в свертке не отличался по размеру от ввода. \n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "#Теперь выровняйте до 1D, примените Плотный -> ReLU (с выпадением) -> softmax\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes, activation='softmax')(drop_3)\n",
    "model = Model(inputs=inp, outputs=out) # Чтобы определить модель, просто укажите ее входные и выходные слои\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_split=0.1) # \n",
    "model.evaluate(X_test, Y_test, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8557f67-cb0d-4a0b-acc3-4fd5edf32e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "num_epochs = 200 \n",
    "kernel_size = 3 #  задающий высоту и ширину окна 2D свертки: мы будем использовать ядра 3x3  (размер ядра в сверточных слоях;)\n",
    "pool_size = 2 # целое число, размер максимального окна пула: мы будем использовать объединение 2х2 повсюду  (размер подвыборки в слоях подвыборки;)\n",
    "conv_depth_1 = 32 # глубина 32 слоя  (количество ядер в сверточных слоях;)\n",
    "conv_depth_2 = 64 # глубина 64\n",
    "drop_prob_1 = 0.25 # выбывает после объединения с вероятностью 0,25   мы будем применять dropout после каждого слоя подвыборки, а также после полносвязного слоя;\n",
    "drop_prob_2 = 0.5 # выпадают в плотном слое с вероятностью 0,5\n",
    "hidden_size = 512 # в полносвязном  слое будет 512 нейронов \n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() # извлечение данных CIFAR-10\n",
    "num_train, depth, height, width = X_train.shape # в CIFAR-10 содержится 50000 обучающих примеров\n",
    "num_test = X_test.shape[0] # в CIFAR-10 содержится 10000 тестовых примеров\n",
    "num_classes = np.unique(y_train).shape[0] # существует 10 классов изображений\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "X_test /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) # кодирование тренировачных данных\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) # кодирование тестировочных данных\n",
    "\n",
    "\n",
    "inp = Input(shape=(depth, height, width)) # обратите внимание, глубина занимает первое место в Керасе\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(inp) #устарело\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "#drop_1 = Dropout(drop_prob_1)(pool_1)  #Регуляризация – это любая модификация алгоритма обучения, предпринятая с целью уменьшить его ошибку обобщения, не уменьшая ошибку обучения.\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(pool_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size),padding='same')(conv_4) # padding='same' входное изображение должно иметь нулевое заполнение, чтобы вывод в свертке не отличался по размеру от ввода. \n",
    "#drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "#Теперь выровняйте до 1D, примените Плотный -> ReLU (с выпадением) -> softmax\n",
    "flat = Flatten()(pool_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "#drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes, activation='softmax')(hidden)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out) # Чтобы определить модель, просто укажите ее входные и выходные слои\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_split=0.1) # verbose=1 - индикатор выполнения\n",
    "#model.evaluate прогнозирует значения и вычисляет потери и все прикрепленные метрики к модели по заданному набору данных. \n",
    "model.evaluate(X_test, Y_test, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1784fbd-7cda-4634-a4f2-db90333cac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "num_epochs = 200 \n",
    "kernel_size = 3 # мы будем использовать ядра 3x3 повсюду\n",
    "pool_size = 2 # мы будем использовать объединение 2х2 повсюду\n",
    "conv_depth_1 = 64 # первоначально у нас будет 32 ядра на слой преобразования\n",
    "conv_depth_2 = 128 # переключение на 64 после первого уровня объединения\n",
    "drop_prob_1 = 0.25 # выбывает после объединения с вероятностью 0,25\n",
    "drop_prob_2 = 0.5 # выпадают в плотном слое с вероятностью 0,5\n",
    "hidden_size = 512 # в плотном слое будет 512 нейронов\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() # извлечение данных CIFAR-10\n",
    "num_train, depth, height, width = X_train.shape # в CIFAR-10 содержится 50000 обучающих примеров\n",
    "num_test = X_test.shape[0] # в CIFAR-10 содержится 10000 тестовых примеров\n",
    "num_classes = np.unique(y_train).shape[0] # существует 10 классов изображений\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "X_test /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) # кодирование тренировачных данных\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) # кодирование тестировочных данных\n",
    "\n",
    "\n",
    "inp = Input(shape=(depth, height, width)) # N.B. глубина занимает первое место в Керасе\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(inp) #устарело\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size),padding='same')(conv_4) # padding='same' входное изображение должно иметь нулевое заполнение, чтобы вывод в свертке не отличался по размеру от ввода. \n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "#Теперь выровняйте до 1D, примените Плотный -> ReLU (с выпадением) -> softmax\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes, activation='softmax')(drop_3)\n",
    "model = Model(inputs=inp, outputs=out) # Чтобы определить модель, просто укажите ее входные и выходные слои\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_split=0.1) # \n",
    "model.evaluate(X_test, Y_test, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76cda5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "num_epochs = 200 \n",
    "kernel_size = 3 # мы будем использовать ядра 3x3 повсюду\n",
    "pool_size = 2 # мы будем использовать объединение 2х2 повсюду\n",
    "conv_depth_1 = 16 # первоначально у нас будет 32 ядра на слой преобразования\n",
    "conv_depth_2 = 32 # переключение на 64 после первого уровня объединения\n",
    "drop_prob_1 = 0.25 # выбывает после объединения с вероятностью 0,25\n",
    "drop_prob_2 = 0.5 # выпадают в плотном слое с вероятностью 0,5\n",
    "hidden_size = 512 # в плотном слое будет 512 нейронов\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() # извлечение данных CIFAR-10\n",
    "num_train, depth, height, width = X_train.shape # в CIFAR-10 содержится 50000 обучающих примеров\n",
    "num_test = X_test.shape[0] # в CIFAR-10 содержится 10000 тестовых примеров\n",
    "num_classes = np.unique(y_train).shape[0] # существует 10 классов изображений\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "X_test /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) # кодирование тренировачных данных\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) # кодирование тестировочных данных\n",
    "\n",
    "\n",
    "inp = Input(shape=(depth, height, width)) # N.B. глубина занимает первое место в Керасе\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(inp) #устарело\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size),padding='same')(conv_4) # padding='same' входное изображение должно иметь нулевое заполнение, чтобы вывод в свертке не отличался по размеру от ввода. \n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "#Теперь выровняйте до 1D, примените Плотный -> ReLU (с выпадением) -> softmax\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes, activation='softmax')(drop_3)\n",
    "model = Model(inputs=inp, outputs=out) # Чтобы определить модель, просто укажите ее входные и выходные слои\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_split=0.1) # \n",
    "model.evaluate(X_test, Y_test, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f452c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "num_epochs = 200 \n",
    "kernel_size = 3 # мы будем использовать ядра 3x3 повсюду\n",
    "pool_size = 2 # мы будем использовать объединение 2х2 повсюду\n",
    "conv_depth_1 = 128 # первоначально у нас будет 32 ядра на слой преобразования\n",
    "conv_depth_2 = 256 # переключение на 64 после первого уровня объединения\n",
    "drop_prob_1 = 0.25 # выбывает после объединения с вероятностью 0,25\n",
    "drop_prob_2 = 0.5 # выпадают в плотном слое с вероятностью 0,5\n",
    "hidden_size = 512 # в плотном слое будет 512 нейронов\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() # извлечение данных CIFAR-10\n",
    "num_train, depth, height, width = X_train.shape # в CIFAR-10 содержится 50000 обучающих примеров\n",
    "num_test = X_test.shape[0] # в CIFAR-10 содержится 10000 тестовых примеров\n",
    "num_classes = np.unique(y_train).shape[0] # существует 10 классов изображений\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "X_test /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) # кодирование тренировачных данных\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) # кодирование тестировочных данных\n",
    "\n",
    "\n",
    "inp = Input(shape=(depth, height, width)) # N.B. глубина занимает первое место в Керасе\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(inp) #устарело\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size),padding='same')(conv_4) # padding='same' входное изображение должно иметь нулевое заполнение, чтобы вывод в свертке не отличался по размеру от ввода. \n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "#Теперь выровняйте до 1D, примените Плотный -> ReLU (с выпадением) -> softmax\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes, activation='softmax')(drop_3)\n",
    "model = Model(inputs=inp, outputs=out) # Чтобы определить модель, просто укажите ее входные и выходные слои\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_split=0.1) # \n",
    "model.evaluate(X_test, Y_test, verbose=1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
