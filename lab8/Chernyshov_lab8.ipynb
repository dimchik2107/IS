{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc446d7d-265d-4eeb-ad7b-242459914164",
   "metadata": {},
   "source": [
    "# Лабораторная работа № 8\n",
    "\n",
    "Генерация текста на основе “Алисы в стране чудес”\n",
    "\n",
    "Выполнил:\n",
    "    Студент группы БФИ1901\n",
    "    Чернышов Дмитрий\n",
    "    \n",
    "Задачи:\n",
    "\n",
    "   1. Ознакомиться с генерацией текста\n",
    "   2. Ознакомиться с системой Callback в Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282579c0",
   "metadata": {},
   "source": [
    "# Цель работы:\n",
    "Рекуррентные нейронные сети также могут быть использованы в качестве генеративных\n",
    "моделей.\n",
    "Это означает, что в дополнение к тому, что они используются для прогнозных моделей\n",
    "(создания прогнозов), они могут изучать последовательности проблемы, а затем\n",
    "генерировать совершенно новые вероятные последовательности для проблемной\n",
    "области.\n",
    "Подобные генеративные модели полезны не только для изучения того, насколько хорошо\n",
    "модель выявила проблему, но и для того, чтобы узнать больше о самой проблемной\n",
    "области."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4394b0da-c0a2-45cf-837b-6c996fdee9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20acf963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144522\n",
      "Total Vocab:  48\n"
     ]
    }
   ],
   "source": [
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "#создаем карту каждого символа с уникальным целым числом (преобразование символов в целые числа)\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "#суммируем набор данных\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "482dcda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144422\n",
      "Total Vocab:  48\n"
     ]
    }
   ],
   "source": [
    "#разделяем текст книги на подпоследовательности с фиксированной длиной\n",
    "#в 100 символов произвольной длины.\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b8ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразовать список входных последовательностей в форму[образцы, временные шаги, особенности]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# Затем нам нужно изменить масштаб целых чисел в диапазоне от 0 до 1,\n",
    "X = X / float(n_vocab)\n",
    "# нужно преобразовать выходные шаблоны (отдельные символы, преобразованные в целые числа) в одну кодировку.\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb38468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.9594\n",
      "Epoch 1: loss improved from inf to 2.95942, saving model to weights-improvement-01-2.9594.hdf5\n",
      "1129/1129 [==============================] - 279s 246ms/step - loss: 2.9594\n",
      "Epoch 2/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.7573\n",
      "Epoch 2: loss improved from 2.95942 to 2.75733, saving model to weights-improvement-02-2.7573.hdf5\n",
      "1129/1129 [==============================] - 278s 246ms/step - loss: 2.7573\n",
      "Epoch 3/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.6551\n",
      "Epoch 3: loss improved from 2.75733 to 2.65511, saving model to weights-improvement-03-2.6551.hdf5\n",
      "1129/1129 [==============================] - 280s 248ms/step - loss: 2.6551\n",
      "Epoch 4/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.5790\n",
      "Epoch 4: loss improved from 2.65511 to 2.57896, saving model to weights-improvement-04-2.5790.hdf5\n",
      "1129/1129 [==============================] - 279s 247ms/step - loss: 2.5790\n",
      "Epoch 5/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.5177\n",
      "Epoch 5: loss improved from 2.57896 to 2.51767, saving model to weights-improvement-05-2.5177.hdf5\n",
      "1129/1129 [==============================] - 300s 266ms/step - loss: 2.5177\n",
      "Epoch 6/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.4629\n",
      "Epoch 6: loss improved from 2.51767 to 2.46294, saving model to weights-improvement-06-2.4629.hdf5\n",
      "1129/1129 [==============================] - 297s 263ms/step - loss: 2.4629\n",
      "Epoch 7/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.4122\n",
      "Epoch 7: loss improved from 2.46294 to 2.41220, saving model to weights-improvement-07-2.4122.hdf5\n",
      "1129/1129 [==============================] - 358s 317ms/step - loss: 2.4122\n",
      "Epoch 8/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.3668\n",
      "Epoch 8: loss improved from 2.41220 to 2.36677, saving model to weights-improvement-08-2.3668.hdf5\n",
      "1129/1129 [==============================] - 363s 321ms/step - loss: 2.3668\n",
      "Epoch 9/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.3200\n",
      "Epoch 9: loss improved from 2.36677 to 2.32004, saving model to weights-improvement-09-2.3200.hdf5\n",
      "1129/1129 [==============================] - 375s 333ms/step - loss: 2.3200\n",
      "Epoch 10/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.2783\n",
      "Epoch 10: loss improved from 2.32004 to 2.27834, saving model to weights-improvement-10-2.2783.hdf5\n",
      "1129/1129 [==============================] - 363s 321ms/step - loss: 2.2783\n",
      "Epoch 11/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.2379\n",
      "Epoch 11: loss improved from 2.27834 to 2.23785, saving model to weights-improvement-11-2.2379.hdf5\n",
      "1129/1129 [==============================] - 386s 342ms/step - loss: 2.2379\n",
      "Epoch 12/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.1984\n",
      "Epoch 12: loss improved from 2.23785 to 2.19845, saving model to weights-improvement-12-2.1984.hdf5\n",
      "1129/1129 [==============================] - 457s 405ms/step - loss: 2.1984\n",
      "Epoch 13/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.1655\n",
      "Epoch 13: loss improved from 2.19845 to 2.16554, saving model to weights-improvement-13-2.1655.hdf5\n",
      "1129/1129 [==============================] - 337s 298ms/step - loss: 2.1655\n",
      "Epoch 14/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.1337\n",
      "Epoch 14: loss improved from 2.16554 to 2.13373, saving model to weights-improvement-14-2.1337.hdf5\n",
      "1129/1129 [==============================] - 328s 290ms/step - loss: 2.1337\n",
      "Epoch 15/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0940\n",
      "Epoch 15: loss improved from 2.13373 to 2.09402, saving model to weights-improvement-15-2.0940.hdf5\n",
      "1129/1129 [==============================] - 288s 255ms/step - loss: 2.0940\n",
      "Epoch 16/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0593\n",
      "Epoch 16: loss improved from 2.09402 to 2.05930, saving model to weights-improvement-16-2.0593.hdf5\n",
      "1129/1129 [==============================] - 281s 249ms/step - loss: 2.0593\n",
      "Epoch 17/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0313\n",
      "Epoch 17: loss improved from 2.05930 to 2.03134, saving model to weights-improvement-17-2.0313.hdf5\n",
      "1129/1129 [==============================] - 282s 249ms/step - loss: 2.0313\n",
      "Epoch 18/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0039\n",
      "Epoch 18: loss improved from 2.03134 to 2.00385, saving model to weights-improvement-18-2.0039.hdf5\n",
      "1129/1129 [==============================] - 276s 244ms/step - loss: 2.0039\n",
      "Epoch 19/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.9763\n",
      "Epoch 19: loss improved from 2.00385 to 1.97635, saving model to weights-improvement-19-1.9763.hdf5\n",
      "1129/1129 [==============================] - 274s 243ms/step - loss: 1.9763\n",
      "Epoch 20/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.9477\n",
      "Epoch 20: loss improved from 1.97635 to 1.94765, saving model to weights-improvement-20-1.9477.hdf5\n",
      "1129/1129 [==============================] - 279s 247ms/step - loss: 1.9477\n",
      "Epoch 21/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.9228\n",
      "Epoch 21: loss improved from 1.94765 to 1.92279, saving model to weights-improvement-21-1.9228.hdf5\n",
      "1129/1129 [==============================] - 280s 248ms/step - loss: 1.9228\n",
      "Epoch 22/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.9014\n",
      "Epoch 22: loss improved from 1.92279 to 1.90138, saving model to weights-improvement-22-1.9014.hdf5\n",
      "1129/1129 [==============================] - 280s 248ms/step - loss: 1.9014\n",
      "Epoch 23/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.8749\n",
      "Epoch 23: loss improved from 1.90138 to 1.87491, saving model to weights-improvement-23-1.8749.hdf5\n",
      "1129/1129 [==============================] - 279s 247ms/step - loss: 1.8749\n",
      "Epoch 24/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.8531\n",
      "Epoch 24: loss improved from 1.87491 to 1.85309, saving model to weights-improvement-24-1.8531.hdf5\n",
      "1129/1129 [==============================] - 278s 246ms/step - loss: 1.8531\n",
      "Epoch 25/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.8341\n",
      "Epoch 25: loss improved from 1.85309 to 1.83413, saving model to weights-improvement-25-1.8341.hdf5\n",
      "1129/1129 [==============================] - 335s 297ms/step - loss: 1.8341\n",
      "Epoch 26/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.8170\n",
      "Epoch 26: loss improved from 1.83413 to 1.81702, saving model to weights-improvement-26-1.8170.hdf5\n",
      "1129/1129 [==============================] - 358s 317ms/step - loss: 1.8170\n",
      "Epoch 27/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.8015\n",
      "Epoch 27: loss improved from 1.81702 to 1.80147, saving model to weights-improvement-27-1.8015.hdf5\n",
      "1129/1129 [==============================] - 367s 325ms/step - loss: 1.8015\n",
      "Epoch 28/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7805\n",
      "Epoch 28: loss improved from 1.80147 to 1.78053, saving model to weights-improvement-28-1.7805.hdf5\n",
      "1129/1129 [==============================] - 337s 299ms/step - loss: 1.7805\n",
      "Epoch 29/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7657\n",
      "Epoch 29: loss improved from 1.78053 to 1.76568, saving model to weights-improvement-29-1.7657.hdf5\n",
      "1129/1129 [==============================] - 338s 299ms/step - loss: 1.7657\n",
      "Epoch 30/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7520\n",
      "Epoch 30: loss improved from 1.76568 to 1.75195, saving model to weights-improvement-30-1.7520.hdf5\n",
      "1129/1129 [==============================] - 344s 305ms/step - loss: 1.7520\n",
      "Epoch 31/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7369\n",
      "Epoch 31: loss improved from 1.75195 to 1.73686, saving model to weights-improvement-31-1.7369.hdf5\n",
      "1129/1129 [==============================] - 345s 306ms/step - loss: 1.7369\n",
      "Epoch 32/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7278\n",
      "Epoch 32: loss improved from 1.73686 to 1.72780, saving model to weights-improvement-32-1.7278.hdf5\n",
      "1129/1129 [==============================] - 314s 278ms/step - loss: 1.7278\n",
      "Epoch 33/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7780\n",
      "Epoch 33: loss did not improve from 1.72780\n",
      "1129/1129 [==============================] - 305s 270ms/step - loss: 1.7780\n",
      "Epoch 34/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7029\n",
      "Epoch 34: loss improved from 1.72780 to 1.70289, saving model to weights-improvement-34-1.7029.hdf5\n",
      "1129/1129 [==============================] - 353s 313ms/step - loss: 1.7029\n",
      "Epoch 35/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6849\n",
      "Epoch 35: loss improved from 1.70289 to 1.68490, saving model to weights-improvement-35-1.6849.hdf5\n",
      "1129/1129 [==============================] - 444s 394ms/step - loss: 1.6849\n",
      "Epoch 36/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6748\n",
      "Epoch 36: loss improved from 1.68490 to 1.67482, saving model to weights-improvement-36-1.6748.hdf5\n",
      "1129/1129 [==============================] - 608s 538ms/step - loss: 1.6748\n",
      "Epoch 37/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6735\n",
      "Epoch 37: loss improved from 1.67482 to 1.67354, saving model to weights-improvement-37-1.6735.hdf5\n",
      "1129/1129 [==============================] - 408s 362ms/step - loss: 1.6735\n",
      "Epoch 38/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6753\n",
      "Epoch 38: loss did not improve from 1.67354\n",
      "1129/1129 [==============================] - 434s 384ms/step - loss: 1.6753\n",
      "Epoch 39/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6653\n",
      "Epoch 39: loss improved from 1.67354 to 1.66529, saving model to weights-improvement-39-1.6653.hdf5\n",
      "1129/1129 [==============================] - 396s 351ms/step - loss: 1.6653\n",
      "Epoch 40/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6514\n",
      "Epoch 40: loss improved from 1.66529 to 1.65139, saving model to weights-improvement-40-1.6514.hdf5\n",
      "1129/1129 [==============================] - 389s 345ms/step - loss: 1.6514\n",
      "Epoch 41/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6458\n",
      "Epoch 41: loss improved from 1.65139 to 1.64583, saving model to weights-improvement-41-1.6458.hdf5\n",
      "1129/1129 [==============================] - 440s 390ms/step - loss: 1.6458\n",
      "Epoch 42/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6393\n",
      "Epoch 42: loss improved from 1.64583 to 1.63927, saving model to weights-improvement-42-1.6393.hdf5\n",
      "1129/1129 [==============================] - 471s 417ms/step - loss: 1.6393\n",
      "Epoch 43/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6314\n",
      "Epoch 43: loss improved from 1.63927 to 1.63140, saving model to weights-improvement-43-1.6314.hdf5\n",
      "1129/1129 [==============================] - 437s 387ms/step - loss: 1.6314\n",
      "Epoch 44/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6242\n",
      "Epoch 44: loss improved from 1.63140 to 1.62423, saving model to weights-improvement-44-1.6242.hdf5\n",
      "1129/1129 [==============================] - 356s 315ms/step - loss: 1.6242\n",
      "Epoch 45/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6129\n",
      "Epoch 45: loss improved from 1.62423 to 1.61287, saving model to weights-improvement-45-1.6129.hdf5\n",
      "1129/1129 [==============================] - 399s 354ms/step - loss: 1.6129\n",
      "Epoch 46/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6067\n",
      "Epoch 46: loss improved from 1.61287 to 1.60673, saving model to weights-improvement-46-1.6067.hdf5\n",
      "1129/1129 [==============================] - 374s 332ms/step - loss: 1.6067\n",
      "Epoch 47/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.5956\n",
      "Epoch 47: loss improved from 1.60673 to 1.59555, saving model to weights-improvement-47-1.5956.hdf5\n",
      "1129/1129 [==============================] - 396s 351ms/step - loss: 1.5956\n",
      "Epoch 48/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.5913\n",
      "Epoch 48: loss improved from 1.59555 to 1.59128, saving model to weights-improvement-48-1.5913.hdf5\n",
      "1129/1129 [==============================] - 388s 344ms/step - loss: 1.5913\n",
      "Epoch 49/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.5950\n",
      "Epoch 49: loss did not improve from 1.59128\n",
      "1129/1129 [==============================] - 397s 352ms/step - loss: 1.5950\n",
      "Epoch 50/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.5959\n",
      "Epoch 50: loss did not improve from 1.59128\n",
      "1129/1129 [==============================] - 394s 349ms/step - loss: 1.5959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19f24b22920>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Из-за медлительности и из-за наших требований по\n",
    "#оптимизации мы будем использовать контрольные точки модели для записи всех сетевых\n",
    "#весов, чтобы каждый раз регистрировать улучшение потерь в конце эпохи.\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
    "verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(X, y, epochs=50, batch_size=128,\n",
    "callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e1698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" rush, and had just begun 'well, of all the unjust\n",
      "things--' when his eye chanced to fall upon alice, \"\n",
      " then she was now agaun to the tanle, and she sett on  so the sere thine was a luttee of the rore of the tarle of the tarle, and the war aoiineed to tai it auay inck the had aooedde  and the wuile of the werl white sae iewting the har and the sinllder at the wast on, 'a-dad aell tire it  and the seie whin saye gard aedore the sabbit wored ball whth the rabei, and she woile rabeed to be salken time tha was oo the thidg on  and the wert ont lr the seales  the was alliered thrh the wuide hu was arl anoier.\n",
      "\n",
      "'thet would not,' said the katter anded, aadan ou hir eeed woth a saik. and she sene the wasted a little sire ti thene sar soon it  and the said th the wurle.\n",
      "and saed to the tueen, the was soi ant lort ani aroinrsing, \n",
      "atice tas the mirtle white rabbit wurld hes as she shile tab itir the har hor io a poeen ti thine say ari a lange harger oeme the white rabbit was so the toie. the was notting so aenit it, and sae to the korke su teal the oage of the goorh, and was going oo hir fned  'i\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# загружаем данные и определяем сеть точно таким же образом, за\n",
    "#исключением того, что веса сети загружаются из файла контрольных точек, и сеть не нуждается в обучении.\n",
    "filename = \"weights-improvement-48-1.5913.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam')\n",
    "#также необходимо создать обратное отображение, которое мы можем использовать для\n",
    "#преобразования целых чисел обратно в символы, чтобы мы могли понять предсказания.\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Выбираем случайный шаблон ввода в качестве начальной последовательности\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# печатаем сгенерированные символы\n",
    "for i in range(1000):\n",
    "\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d456fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" s so large a house, that she did not like to go nearer till she had\n",
      "nibbled some more of the lefthan \"\n",
      "s was so sie toid. and the west on  gnt it was an the winde sabdi  she was aoling toe piget her hrok alange th the terl to her oo the saale, and she soie let head out the hoos  sor ieeds the rabbit was in asllhets  and then she wal allcere that she was aolthe  bui it sas an the wan ou toe ti the thele  bnin the latthrs was the was toi tinl to sar the siaee, and the west on aroiersdy at the wasted out of the woid, and sae to the kotke su tee then shee ohe head out the harce of the caokse, and she taited out of the woide so see the had hor no the taale, and the white rabbit was the winte rabbit, and the wert on ar all thite was ao allc  and sar nort blice, and sae to thin his dead oo the toeee. \n",
      "'the surtldd thing,  said the goyphon, 'i wesl to the whitenr!'\n",
      "\n",
      "'i movt s gn as all ' said alice, whr was sore aiained tore  \n",
      "and the sere thin sire she seae tuine oot an all cor oo the danken, and sal to the wonde su saa it ani the whil  at the same th the word tf the seale, and saeded th the w\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e8c7c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144522\n",
      "Total Vocab:  48\n",
      "Total Patterns:  144422\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "Epoch 1/10\n",
      "283/283 [==============================] - 331s 1s/step - loss: 3.0596\n",
      "Epoch 2/10\n",
      "283/283 [==============================] - 330s 1s/step - loss: 2.9349\n",
      "Epoch 3/10\n",
      "283/283 [==============================] - 324s 1s/step - loss: 2.8365\n",
      "Epoch 4/10\n",
      "283/283 [==============================] - 324s 1s/step - loss: 2.7785\n",
      "Epoch 5/10\n",
      "283/283 [==============================] - 322s 1s/step - loss: 2.7356\n",
      "Epoch 6/10\n",
      "283/283 [==============================] - 303s 1s/step - loss: 2.6966\n",
      "Epoch 7/10\n",
      "283/283 [==============================] - 303s 1s/step - loss: 2.6547\n",
      "Epoch 8/10\n",
      "283/283 [==============================] - 301s 1s/step - loss: 2.6158\n",
      "Epoch 9/10\n",
      "283/283 [==============================] - 298s 1s/step - loss: 2.5834\n",
      "Epoch 10/10\n",
      "283/283 [==============================] - 298s 1s/step - loss: 2.5509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19f5ba87160>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32,\n",
    "                                             write_graph=True, write_grads=False, write_images=False,\n",
    "                                             embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None,\n",
    "                                             embeddings_data=None, update_freq='epoch')\n",
    "\n",
    "model.fit(X, y, epochs=10, batch_size=512, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50579eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-238247e5cc90d247\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-238247e5cc90d247\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the network weights\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69348a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144522\n",
      "Total Vocab:  48\n",
      "Total Patterns:  144422\n",
      "Epoch 1/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 3.0619Seed:\n",
      "\" oice.\n",
      "\n",
      "'back to land again, and that's all the first figure,' said the mock\n",
      "turtle, suddenly droppin \"\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "Done.\n",
      "283/283 [==============================] - 335s 1s/step - loss: 3.0619\n",
      "Epoch 2/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.9453Seed:\n",
      "\" re nowhere to be found: all she could see, when she looked down, was\n",
      "an immense length of neck, whic \"\n",
      " to th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th \n",
      "Done.\n",
      "283/283 [==============================] - 336s 1s/step - loss: 2.9453\n",
      "Epoch 3/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.8383Seed:\n",
      "\" ur pardon!' said the mouse, frowning, but very politely: 'did\n",
      "you speak?'\n",
      "\n",
      "'not i!' said the lory ha \"\n",
      " the toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe\n",
      "Done.\n",
      "283/283 [==============================] - 355s 1s/step - loss: 2.8383\n",
      "Epoch 4/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.7743Seed:\n",
      "\" gether alice did not\n",
      "like the look of the thing at all. 'but perhaps it was only sobbing,'\n",
      "she thoug \"\n",
      " toe  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and \n",
      "Done.\n",
      "283/283 [==============================] - 386s 1s/step - loss: 2.7743\n",
      "Epoch 5/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.7283Seed:\n",
      "\" ld the fall never come to an end! 'i wonder how\n",
      "many miles i've fallen by this time?' she said aloud \"\n",
      " toe  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and \n",
      "Done.\n",
      "283/283 [==============================] - 344s 1s/step - loss: 2.7283\n",
      "Epoch 6/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.6807Seed:\n",
      "\" d 'no, never') '--so you can have no idea what a delightful thing a\n",
      "lobster quadrille is!'\n",
      "\n",
      "'no, ind \"\n",
      " toe  as ce a sant to tee  and the  an ce an a sore toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  and the toet toe toet to the  a\n",
      "Done.\n",
      "283/283 [==============================] - 347s 1s/step - loss: 2.6807\n",
      "Epoch 7/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.6340Seed:\n",
      "\" s! how brave they'll all think me at\n",
      "home! why, i wouldn't say anything about it, even if i fell off \"\n",
      " toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the  and the toet the \n",
      "Done.\n",
      "283/283 [==============================] - 338s 1s/step - loss: 2.6340\n",
      "Epoch 8/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.5968Seed:\n",
      "\" ound the court\n",
      "with a smile. there was a dead silence.\n",
      "\n",
      "'it's a pun!' the king added in an offended  \"\n",
      " 'nh the toet toet toe toet to tee   shi  alice aare to the woet  and the toet toe toet toet the toet the toet the toet  and the toet the toet the toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the toet toe toet the toet  and the toet the t\n",
      "Done.\n",
      "283/283 [==============================] - 342s 1s/step - loss: 2.5968\n",
      "Epoch 9/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.5633Seed:\n",
      "\" in to the beginning of the conversation.\n",
      "alice felt a little irritated at the caterpillar's making s \"\n",
      "o the woee  and the woet  and the toet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the \n",
      "Done.\n",
      "283/283 [==============================] - 345s 1s/step - loss: 2.5633\n",
      "Epoch 10/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.5325Seed:\n",
      "\" te down on their slates, 'she doesn't believe there's an\n",
      "atom of meaning in it,' but none of them at \"\n",
      " al cn the toee to the toee to the toee to the tast te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart te the tart \n",
      "Done.\n",
      "283/283 [==============================] - 344s 1s/step - loss: 2.5325\n",
      "Epoch 11/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.5010Seed:\n",
      "\" rted violently, dropped the white kid gloves and the fan,\n",
      "and skurried away into the darkness as har \"\n",
      " sare the woeee to the woee  \n",
      "'i whe toue   shi gateer hare and the woeee  'nh the toet toe woee to tee toe toee to the toee to the toee to the toee to the woee  \n",
      "'i whe toue   shi gateer hare and the woeee  'nh the toet toe woee to tee toe toee to the toee to the toee to the toee to the woee  \n",
      "'i whe toue   shi gateer hare and the woeee  'nh the toet toe woee to tee toe toee to the toee to the toee to the toee to the woee  \n",
      "'i whe toue   shi gateer hare and the woeee  'nh the toet toe woee to tee toe toee to the toee to the toee to the toee to the woee  \n",
      "'i whe toue   shi gateer hare and the woeee  'nh the toet toe woee to tee toe toee to the toee to the toee to the toee to the woee  \n",
      "'i whe toue   shi gateer hare and the woeee  'nh the toet toe woee to tee toe toee to the toee to the toee to the toee to the woee  \n",
      "'i whe toue   shi gateer hare and the woeee  'nh the toet toe woee to tee toe toee to the toee to the toee to the toee to the woee  \n",
      "'i whe toue   shi gateer hare and the w\n",
      "Done.\n",
      "283/283 [==============================] - 341s 1s/step - loss: 2.5010\n",
      "Epoch 12/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.4715Seed:\n",
      "\"  it; but, as\n",
      "the door opened inwards, and alice's elbow was pressed hard against it,\n",
      "that attempt pr \"\n",
      " the toee to the toeee  and the woene toee to tee toee to the toee to the toee to the toeee  and the woene toee to tee toee to the toeee  and the woene to the toee to the toee to the toee to the toeee  and the woene toee to tee toee to the toeee  and the woene to the toee to the toee to the toee to the toeee  and the woene toee to tee toee to the toeee  and the woene to the toee to the toee to the toee to the toeee  and the woene toee to tee toee to the toeee  and the woene to the toee to the toee to the toee to the toeee  and the woene toee to tee toee to the toeee  and the woene to the toee to the toee to the toee to the toeee  and the woene toee to tee toee to the toeee  and the woene to the toee to the toee to the toee to the toeee  and the woene toee to tee toee to the toeee  and the woene to the toee to the toee to the toee to the toeee  and the woene toee to tee toee to the toeee  and the woene to the toee to the toee to the toee to the toeee  and the woene toee to tee toee to t\n",
      "Done.\n",
      "283/283 [==============================] - 342s 1s/step - loss: 2.4715\n",
      "Epoch 13/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.4453Seed:\n",
      "\"  northumbria, declared for him: and even stigand,\n",
      "the patriotic archbishop of canterbury, found it a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lattee  and the tas in a lattee  and the toee to the tast oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io the tart oo the taate  and the tai io \n",
      "Done.\n",
      "283/283 [==============================] - 345s 1s/step - loss: 2.4453\n",
      "Epoch 14/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.4156Seed:\n",
      "\" y sobbing of the mock turtle. alice was very nearly getting up and\n",
      "saying, 'thank you, sir, for your \"\n",
      " ho whe woee to the toee to the toee  \n",
      "\n",
      "'io tou d vont dade ' said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "'the mone th the toue   said alice. \n",
      "\n",
      "Done.\n",
      "283/283 [==============================] - 340s 1s/step - loss: 2.4156\n",
      "Epoch 15/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.3978Seed:\n",
      "\" ained some time after the rest of it had gone.\n",
      "\n",
      "'well! i've often seen a cat without a grin,' though \"\n",
      "t alice, ''that a souu oo the toee to the toee '\n",
      "\n",
      "'io toued to tee soat ' said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   said the caterpillar.\n",
      "\n",
      "'that s the toie   sa\n",
      "Done.\n",
      "283/283 [==============================] - 341s 1s/step - loss: 2.3978\n",
      "Epoch 16/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.3659Seed:\n",
      "\" las! either the locks were too large, or the key was too small,\n",
      "but at any rate it would not open an \"\n",
      " cnre an an cn whe wooe  \n",
      "''                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "Done.\n",
      "283/283 [==============================] - 346s 1s/step - loss: 2.3659\n",
      "Epoch 17/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.3405Seed:\n",
      "\" ght it, and kept doubling itself up and\n",
      "straightening itself out again, so that altogether, for the  \"\n",
      "woene wab in sas anlne the woeee oate the soeee of the ciute  and the woene was oot in the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  and tas anl toen she woene was oo the toeee  an\n",
      "Done.\n",
      "283/283 [==============================] - 342s 1s/step - loss: 2.3405\n",
      "Epoch 18/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.3151Seed:\n",
      "\"  it'll seem, sending\n",
      "presents to one's own feet! and how odd the directions will look!\n",
      "\n",
      "     alice's \"\n",
      "eil in toe cade--                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "Done.\n",
      "283/283 [==============================] - 342s 1s/step - loss: 2.3151\n",
      "Epoch 19/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.2863Seed:\n",
      "\" r\n",
      "finger very deeply with a knife, it usually bleeds; and she had never\n",
      "forgotten that, if you drink \"\n",
      " to tee soee  \n",
      "\n",
      "'i me toen ii tou dene to tea so tee soat,' said the monk turtle  'so the sooe   she gate rard to aerce  and toen to tee toeee to the toeen  and the sart on tert an anl the was an the cad no the care aad not thet she woene was an anl toe was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl the was an anl \n",
      "Done.\n",
      "283/283 [==============================] - 345s 1s/step - loss: 2.2863\n",
      "Epoch 20/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.2636Seed:\n",
      "\"  the words.'\n",
      "\n",
      "so they began solemnly dancing round and round alice, every now and\n",
      "then treading on h \"\n",
      "er head  and the sast oo tiat the wooee tas oo tee that she was hot ano anr and the wait on tee thit har en she cad nete the har en an soee  and the said thi was sot dn toe to the white tabbit  and the said thi gad nete the woeee of the ciuter of the ciute   the karter was an in satt toee i fat an ane a lirtee oo the sast of the care rate the har en an soee  and the sast on teat the woiee oate the soeer th the to the tai iot an in shth the har en an shel  and the sast on teat the woine tas in a lott of the sabte  and tas an in satt toee to the toeer oo the white tabbit  and the sas ant toe taati to tee the was hot an an ooee a aeteerill of the cirte   the karter waa in a vore of thete tas an in soted the had bete th the toeee oe the care ratt anrner thet sae io the care rate the har en an anled and the was oot oo the thate  and tas an in satt toen the was oo tae io the hirtee oo the white tabbit  and the sas ant toe taati to tee the was hot an an ooee a aetterill oo the tas hn the care\n",
      "Done.\n",
      "283/283 [==============================] - 353s 1s/step - loss: 2.2636\n",
      "Epoch 21/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.2370Seed:\n",
      "\" everything seemed to have changed since her swim in\n",
      "the pool, and the great hall, with the glass tab \"\n",
      "t the was anl toen  and the sart on tore th the sooe  and the sart on tore th the sooe  and the sart on tore th the sooe  and the sas aol toee to the thete tas an in shte the har en an anrers on the tast of the care and the wooue tabbit whet see so the toee to the thoee tar oo the tooee the was oo the thse  and the tart on toet the whi gad not the tooe  afd toer te toee to the toie   \n",
      "'i th tet an in wai so thi so to the thate ' said the matter, 'in soe te the soie tf the thet the woine to the thit hare '\n",
      "\n",
      "'io tou d voot do a latter ' said the monk. 'the mart ro toe thatee to the thit wh the to the thaten  the horr the marter shre the had   the said th thr soied in the tooee th the sabtin  and the sas aol toee to the thete  and the sast on tore th the sooe  and the sart on tore th the sooe  and the tart on toet th the thaten  the soee to the sart of the saatin  b dort of the saatin th the th the sooe   the said th herself, 'io soet th toe to the toie to the toie ' \n",
      "'i me toted to toe t\n",
      "Done.\n",
      "283/283 [==============================] - 348s 1s/step - loss: 2.2370\n",
      "Epoch 22/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.2127Seed:\n",
      "\" emember where.'\n",
      "\n",
      "'well, it must be removed,' said the king very decidedly, and he called\n",
      "the queen,  \"\n",
      "and then the was gotn the thaten  the wosed the whine tabbit whet ser to the that she was hotn the sooe  and the sard thin the was not an the woole  and then the was hotn the sooe  bnd the said th the saati gare the shete tabting th the sooee the had fete the wooee th the shre oh the har en a lort oa date ral io hir hend, and the was aolin to the thite tabbit  and the wai io a lott oaad to the white wab it was an the cad nete the har en a lort of the theee  and the sast on tea io the whse an an anleer, and the was aol the war  afd toen th tee the harter  a dort oo the saatin  bnd the sas   the har hn a lort whit sorel oh the sooee   the said thit  she woene wab in a dort of the soree the had fete the wooee th the shre oh the har en a lort oa date ral io hir hend, and the was aolin to the thite tabbit  and the wai io a lott oaad to the white wab it was an the cad nete the har en a lort of the theee  and the sast on tea io the whse an an anleer, and the was aol the war  afd toen th tee t\n",
      "Done.\n",
      "283/283 [==============================] - 350s 1s/step - loss: 2.2127\n",
      "Epoch 23/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.1893Seed:\n",
      "\" ar sternly. 'explain\n",
      "yourself!'\n",
      "\n",
      "'i can't explain myself, i'm afraid, sir' said alice, 'because i'm  \"\n",
      "not the sooe   she said to herself, 'i mo toin i voon the that she soeet so the sooe ' \n",
      "'i te toie the rabtin a dat ' said the more, 'no s a gort thi grrsoen   \n",
      "'io wou dad not io wo tea sh the sooe     ie tou whit hare te the waate      bedutiful  bo cele ffr oo toe \n",
      "     beauteful  bo cnlefus sould \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "Done.\n",
      "283/283 [==============================] - 348s 1s/step - loss: 2.1893\n",
      "Epoch 24/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.1642Seed:\n",
      "\"  shifting\n",
      "from one foot to the other, looking uneasily at the queen, and in\n",
      "his confusion he bit a l \"\n",
      "atde hare aadin  and the whit hare an toe ooatt oo the saatin oo the saster of the care an an cale oo the tooe, and the sar aol aro aeri an anl aari anl ano ano ano ano ano ano ano aeain, and the sas anl the wai io tas toin ht the thene  and whe suoen oas an the cad betn that she was anlnne to the thete tat in a more of thece, and the wai iot in a little to bn at the cadl oo the cane,\n",
      "\n",
      "'thel toe was a little soreee to thi toee to the soeet ' \n",
      "'i me toted toe cane oittie   said the caterpillar.\n",
      "\n",
      "'lel a aanan a gittee toie,' said the mock turtle. \n",
      "'io wou d voued the car a satter ' said the more, 'no see seat the cad betn thet thee to the toen to the toee to the soeen. and the wai iot in a little to bn at the cadl oo the cane,\n",
      "\n",
      "'thel toe was a little soreee to thi toee to the soeet ' \n",
      "'i me toted toe cane oittie   said the caterpillar.\n",
      "\n",
      "'lel a aanan a gittee toie,' said the mock turtle. \n",
      "'io wou d voued the car a satter ' said the more, 'no see seat the cad betn thet thee to the toen to \n",
      "Done.\n",
      "283/283 [==============================] - 345s 1s/step - loss: 2.1642\n",
      "Epoch 25/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.1382Seed:\n",
      "\" ary ways of\n",
      "living would be like, but it puzzled her too much, so she went on: 'but\n",
      "why did they liv \"\n",
      "er tae in a soee '\n",
      "\n",
      "'i cen t teoe the moreo sf the soee,' said the caterpillar.\n",
      "\n",
      "'iele you mane the garter ' said the cate pirlied \n",
      "'io a sard thi grrsouse ' sheu hage an alicd. \n",
      "'thel   shi motgh taid to the jury,on the sooe, \n",
      "and the wart on an ince to the thate  and whe suoen was an toee a aoo of the career  a donr thit har so ter an the could, \n",
      "'whe care sait doon thet ' said the marthr, ''that a lort oatee to thit ' said the caterpillar.\n",
      "\n",
      "'iele you mane the garter ' said the cate pirlied \n",
      "'io a sard thi grrsouse ' sheu hage an alicd. \n",
      "'thel   shi motgh taid to the jury,on the sooe, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and the wart on an ince to the thate  and whe suoen was an toee a aoo of the career  a donr thit har so ter an the could, \n",
      "'whe care sait doon thet ' said the marthr, ''that a lort oatee to thit ' said the caterpillar.\n",
      "\n",
      "'iele you mane the garter ' said the cate pirlied \n",
      "'io a sard thi grrsouse ' sheu hage an alicd. \n",
      "'thel   shi motgh taid to the jury,on the sooe, \n",
      "and the wart on an ince to the thate  \n",
      "Done.\n",
      "283/283 [==============================] - 345s 1s/step - loss: 2.1382\n",
      "Epoch 26/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.1173Seed:\n",
      "\"  a dormouse was sitting\n",
      "between them, fast asleep, and the other two were using it as a\n",
      "cushion, res \"\n",
      " an anl aad not on the cir. \n",
      "'the cru'o tooe th toen i aen to the ro thing to teae,' she said to herself, and seiu on an cnl oo the cir. and the was aoo ano der aaad io a lirtle so tho the har end the was so the theee  and was an in satten  \n",
      "\n",
      "'tha sert to hoa'  said the caterpillar.\n",
      "\n",
      "'iede you toolt ' said the konk turtle. 'toe was a lertle soreoe toued an in shiee an in shen shee  aaduus an in saaten to herd to teet th the to the tab if the cane an anle and not in the tooe. \n",
      "'the sest to bene to ae in a satee tai,, said the cothouse, 'io i gen toen i gen to the thie th the to the thit hare ne the saatin to tea in the coose   \n",
      "'io whu thin   said the cothouse, 'no a aareer are toue ' \n",
      "'io  yhu to tein to h aen,' said the monk turtle, ''then io the raa it hane yo the tooe,' \n",
      "'io to the tait to hene ' said the caterpillar.\n",
      "\n",
      "'iede you toolt ' said the konk turtle. ''then toe cart riite ' she said to herself, 'io wou de a vaid to the sooe, \n",
      "\n",
      "'io tould thu go wo tene to tee ' said the monk \n",
      "Done.\n",
      "283/283 [==============================] - 346s 1s/step - loss: 2.1173\n",
      "Epoch 27/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.0934Seed:\n",
      "\" or a\n",
      "minute or two, which gave the pigeon the opportunity of adding, 'you're\n",
      "looking for eggs, i kno \"\n",
      "w that i vas a thil oo toe toen to the toee '\n",
      "\n",
      "'i me toted toen aadut in,  said the monk turtle an anloout tone. 'to the woin io   she had  oo whet world the mabbit sare to toeke to the woene  and the sard thin  she wou d wonl oo gren '\n",
      "\n",
      "'to toat to here to the tait th the to the sooe,' said the caterpillar.\n",
      "\n",
      "'well, then s e vored no woin io whu  and toenk yhu  and toen io whu  and toen io whu  wou d vooe ohe toee '\n",
      "\n",
      "'i te toen io whu hane toted the crewe,' she macc thire reidieed \n",
      "'iot i mo woy doonn the raatin of the sooe-' she said to herself, 'io coo tee toin io the contee of the siatter ' \n",
      "'io would io wou dane toen iine,' said the caterpillar.\n",
      "\n",
      "'ied you mo wound no ho whu ane neneen ' saed the monk turtle, 'to the woine wou dnen toe rabtin of the sooe-' \n",
      "''yhu d voundd tai so hert ho the raaes,' the macg thrseed  'in s a lort dn ar all whu 'a dorshu oo too  wou doow '\n",
      "\n",
      "'i te toin io   shi gatter wert on, 'in i a sote io whe soeet ' \n",
      "'io   said the caterpillar.\n",
      "\n",
      "'ied you mo wound \n",
      "Done.\n",
      "283/283 [==============================] - 349s 1s/step - loss: 2.0934\n",
      "Epoch 28/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.0719Seed:\n",
      "\" y way, being quite unable to move. she soon got\n",
      "it out again, and put it right; 'not that it signifi \"\n",
      " to tha shie the was an in site the mooe of the siitee of the cirtte of the coupouse  aidcn tot the was sott a mott of the soiee of the cabe,\n",
      "\n",
      "'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "Done.\n",
      "283/283 [==============================] - 344s 1s/step - loss: 2.0719\n",
      "Epoch 29/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.0485Seed:\n",
      "\" n.\n",
      "\n",
      "'how should i know?' said alice, surprised at her own courage. 'it's no\n",
      "business of mine.'\n",
      "\n",
      "the  \"\n",
      "was sot loee tored the wai iotn the gins  and the sool  afd tou dane to tel the garter, \n",
      "\n",
      "whe kact hn wat aoonge to toenk that she was anling to tee the wab in wae nereen the care an the sast of the cace, \n",
      "'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "Done.\n",
      "283/283 [==============================] - 347s 1s/step - loss: 2.0485\n",
      "Epoch 30/30\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.0318Seed:\n",
      "\"  a\n",
      "        trial: for\n",
      "      really this\n",
      "     morning i've\n",
      "    nothing\n",
      "    to do.\"\n",
      "     said the\n",
      "     \"\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "Done.\n",
      "283/283 [==============================] - 351s 1s/step - loss: 2.0318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19f5c0aa830>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam')\n",
    "\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "            if (epoch + 1)%1 == 0:\n",
    "                # pick a random seed\n",
    "                start = numpy.random.randint(0, len(dataX)-1)\n",
    "                pattern = dataX[start]\n",
    "                print (\"Seed:\")\n",
    "                print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "    \n",
    "            # generate characters\n",
    "                for i in range(1000):\n",
    "                    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "                    x = x / float(n_vocab)\n",
    "                    prediction = model.predict(x, verbose=0) \n",
    "                    index = numpy.argmax(prediction)\n",
    "                    result = int_to_char[index]\n",
    "                    seq_in = [int_to_char[value] for value in pattern]\n",
    "                    sys.stdout.write(result)\n",
    "                    pattern.append(index)\n",
    "                    pattern = pattern[1:len(pattern)]\n",
    "                    \n",
    "                print (\"\\nDone.\")\n",
    "                \n",
    "# define the checkpoint\n",
    "#tb_callback = keras.callbacks.Callback.CustomCallback (log_dir='./logs', histogram_freq=0, batch_size=32,\n",
    "#write_graph=True, write_grads=False, write_images=False,\n",
    "#embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None,\n",
    "#embeddings_data=None, update_freq='epoch')\n",
    "\n",
    "model.fit(X, y, epochs=30, batch_size=512, callbacks=[CustomCallback()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa4846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
